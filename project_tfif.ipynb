{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_tfif.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UPcDuzneGQi7"
      ],
      "authorship_tag": "ABX9TyN0ti+mHiceKQBB27Zl/tuO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lauraAriasFdez/SentimentAnalysis/blob/main/project_tfif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Connect To Google Drive + Get Data\n"
      ],
      "metadata": {
        "id": "UPcDuzneGQi7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDiR8OkIGMAH",
        "outputId": "2601a9f3-a4ae-408a-b5b8-e4f1f0a0281d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# MAIN DIRECTORY STILL TO DO \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = \"/content/gdrive/MyDrive/CSCI4511W/project/sentiments.csv\""
      ],
      "metadata": {
        "id": "Q8Mii_apGZwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "cols = ['sentiment','id','date','query_string','user','text']\n",
        "sms_data = pd.read_csv(data_file, encoding='latin-1',header=None,names=cols)\n",
        "\n",
        "# replace lables 0 = neg  1= pos\n",
        "sms_data.sentiment = sms_data.sentiment.replace({0: 0, 4: 1})\n",
        "\n",
        "\n",
        "labels = sms_data[sms_data.columns[0]].to_numpy()\n"
      ],
      "metadata": {
        "id": "V1FM52jeGbpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess Data\n"
      ],
      "metadata": {
        "id": "Ew66mVXuHOh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "#We import English stop-words from the NLTK package and removed them if found in the sentence.\n",
        "#While removing stop-words, we perform stemming that is if the word is not a stop-word, it will be converted to its root form. This is called stemming.\n",
        "\n",
        "\"\"\"\n",
        "https://stackoverflow.com/questions/52026677/sentiment140-preprocessing\n",
        "https://www.analyticsvidhya.com/blog/2020/11/understanding-naive-bayes-svm-and-its-implementation-on-spam-sms/\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def clean_data(content):\n",
        "  stemming = PorterStemmer()\n",
        "\n",
        "  for i in range (0,len(content)):\n",
        "\n",
        "    ## print where in cleaning they are\n",
        "    if (i%1000000==0):\n",
        "      print(i ,\" already cleaned\")\n",
        "\n",
        "    #remove @mentions\n",
        "    tweet = re.sub(r'@[A-Za-z0-9]+',\"\",content[i]) \n",
        "    #remove urls\n",
        "    tweet = re.sub(r'https?:\\/\\/\\S+',\"\",tweet) \n",
        "\n",
        "    #remove all unecessary charachters like punctuations\n",
        "    tweet = re.sub('[^a-zA-Z]',repl = ' ',string = tweet)\n",
        "    tweet.lower()\n",
        "    tweet = tweet.split()\n",
        "\n",
        "    ## steeeming and remove stop words\n",
        "    tweet = [stemming.stem(word) for word in tweet if word not in set(stopwords.words('english'))]\n",
        "    tweet = ' '.join(tweet)\n",
        "\n",
        "    #cleaned Twwet\n",
        "    content[i] = tweet\n",
        "  return content\n"
      ],
      "metadata": {
        "id": "R-7-jz2rHQSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516df31d-4789-432f-9cac-8e3badd10885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://getpocket.com/read/3040941140\n",
        "\n",
        " Texthero is designed as a Pandas wrapper, so it makes it easier than ever to preprocess and analyze text based Pandas Series"
      ],
      "metadata": {
        "id": "hUZQlP_2OKLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install texthero\n",
        "import pandas as pd\n",
        "import texthero as hero #config import cid, csec, ua"
      ],
      "metadata": {
        "id": "YvB69_fCUKAe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb7a7d46-4d8b-4980-935b-1268066fc26c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting texthero\n",
            "  Downloading texthero-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.21.5)\n",
            "Collecting unidecode>=1.1.1\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.0.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (2.2.4)\n",
            "Requirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.2.2)\n",
            "Collecting nltk>=3.3\n",
            "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 42.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.0.2)\n",
            "Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (5.5.0)\n",
            "Requirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.5.0)\n",
            "Requirement already satisfied: gensim<4.0,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (1.1.0)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2022.3.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2018.9)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (8.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->texthero) (3.1.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.9.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.10)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud>=1.5.0->texthero) (7.1.2)\n",
            "Installing collected packages: regex, unidecode, nltk, texthero\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.7 regex-2022.3.15 texthero-1.1.0 unidecode-1.3.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk",
                  "regex"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "custom_cleaning = [\n",
        "  \n",
        "  #Replace not assigned values with empty space\n",
        "  hero.preprocessing.fillna,\n",
        "  hero.preprocessing.lowercase,\n",
        "  hero.preprocessing.remove_digits,\n",
        "  hero.preprocessing.remove_punctuation,\n",
        "  hero.preprocessing.remove_diacritics,\n",
        "  hero.preprocessing.remove_stopwords,\n",
        "  hero.preprocessing.remove_whitespace,\n",
        "  hero.preprocessing.stem\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "content = hero.clean(sms_data['text'], pipeline = custom_cleaning)\n"
      ],
      "metadata": {
        "id": "4_6nGN04PTIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#content = content.to_numpy()"
      ],
      "metadata": {
        "id": "TIoHWw_vTGVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF Feature Extraction "
      ],
      "metadata": {
        "id": "_FOYiY-JKDEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_data = tfidf.fit_transform(content)"
      ],
      "metadata": {
        "id": "t5N2N-J7KHre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "tfidf_x_train,tfidf_x_test,y_train,y_test = train_test_split(tfidf_data,labels,test_size = 0.3, stratify=labels,random_state=100)"
      ],
      "metadata": {
        "id": "l6tAReSqKJJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "u3omeok-KSja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# NAIVE BAYES + TLF \n",
        "print(\"NAIVE BAYES + TLF______________________________________________________________\")\n",
        "clf_multinomialnb = MultinomialNB()\n",
        "clf_multinomialnb.fit(tfidf_x_train,y_train)\n",
        "\n",
        "y_pred = clf_multinomialnb.predict(tfidf_x_test)\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "#>>> f1_score(y_true, y_pred, average='weighted')\n",
        "f1_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UL2oQ9BKXvV",
        "outputId": "85135adf-5d65-4118-bc2f-45af0ccc3fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAIVE BAYES + TLF______________________________________________________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.79      0.76    240000\n",
            "           1       0.77      0.72      0.75    240000\n",
            "\n",
            "    accuracy                           0.76    480000\n",
            "   macro avg       0.76      0.76      0.76    480000\n",
            "weighted avg       0.76      0.76      0.76    480000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7475814603720831"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM"
      ],
      "metadata": {
        "id": "7IZXRA6nKia8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# SVM + TLF \n",
        "print(\"LINEAR SVM + TLF______________________________________________________________\")\n",
        "linearsvc = LinearSVC()\n",
        "linearsvc.fit(tfidf_x_train,y_train)\n",
        "y_pred = linearsvc.predict(tfidf_x_test)\n",
        "\n",
        "print(classification_report(y_test,y_pred))\n",
        "f1_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQwM92xpKhoc",
        "outputId": "6df5c407-5b4e-4765-b389-81a9763e931c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LINEAR SVM + TLF______________________________________________________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.75      0.77    240000\n",
            "           1       0.76      0.79      0.77    240000\n",
            "\n",
            "    accuracy                           0.77    480000\n",
            "   macro avg       0.77      0.77      0.77    480000\n",
            "weighted avg       0.77      0.77      0.77    480000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.774352787433251"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression\n"
      ],
      "metadata": {
        "id": "-eGtzr2aK1Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(tfidf_x_train,y_train)\n",
        "\n",
        "y_pred = logisticRegr.predict(tfidf_x_test)\n",
        "\n",
        "print(classification_report(y_test,y_pred))\n",
        "f1_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxnYxefBK_6Q",
        "outputId": "249e559b-a863-4275-dcd0-0d335f5e7a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.75      0.77    240000\n",
            "           1       0.76      0.80      0.78    240000\n",
            "\n",
            "    accuracy                           0.78    480000\n",
            "   macro avg       0.78      0.78      0.78    480000\n",
            "weighted avg       0.78      0.78      0.78    480000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7841143797154724"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NGQKezQGLBJK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}